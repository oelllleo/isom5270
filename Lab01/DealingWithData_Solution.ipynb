{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python and Pandas Data Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Packages and Built-in Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python has a ton of packages that make doing complicated stuff very easy. We won't discuss how to install packages, or give a detailed list of what packages exist, but we will give a brief description about how they are used. \n",
    "\n",
    "An easy way to think of why package are useful is by thinking: \"**Python packages give us access to MANY functions**\".\n",
    "\n",
    "Packages contain pre-defined functions (built-in) that make our life easier!  We've seen pre-defined functions before, for example, the funciton 'str()' that we used to convert numbers into strings in the Python Basics notebook.\n",
    "\n",
    "In this class we will use four packages very frequently: `pandas`, `sklearn`, `matplotlib`, and `numpy`:\n",
    "\n",
    "- **`pandas`** is a data manipulation package. It lets us store data in data frames. More on this soon.\n",
    "- **`sklearn`** is a machine learning and data science package. It lets us do fairly complicated machine learning tasks, such as running regressions and building classification models with only a few lines of code. (Nice!)\n",
    "- **`matplotlib.pyplot`** lets you make plots and graphs directly from your code.  This can be a secret weapon when combined with notebooks, as you can very easily rerun analyses on different data or with slightly different code, and the graphs can just appear magically.  (Ok, always easier said than done, but you get the idea.)\n",
    "- **`seaborn`** an extension to matplotlib that really helps make your plots look more appealing\n",
    "- **`numpy`** (pronounced num-pie) is used for doing \"math stuff\", such as complex mathematical operations (e.g., square roots, exponents, logs), operations on matrices, and more. \n",
    "\n",
    "As we use these through the semester, their usefulness will become increasingly apparent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the contents of a package available, you need to import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import sklearn\n",
    "import matplotlib.pyplot\n",
    "import numpy\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is easier to use short names for packages. This has become the norm now, so let's do it so that you recognize it if you encounter it in your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# just some stylistic tweaks in seaborn\n",
    "sns.set(style='ticks', palette='Set2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use package-specific things. For example, numpy has a function called `sqrt()` which will give us the square root of a numpy number. Since it is part of numpy, we need to tell Python that that's where it is by using a dot (e.g., `np.sqrt()`).\n",
    "\n",
    "In the following cell you can also see how to write **comments** in your code. Take my advice: write comments as you go.  It's helpful when you want to collaborate, then you don't have to figure out what you did to explain it to your collaborator.  But even more: often you need to come back to an analysis weeks, months, or even years later, and you will thank yourself for explaining what you did!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "some_list = [0,0,1,2,3,3,4.5,7.6]\n",
    "some_dictionary = {'student1': '(929)-000-0000', 'student2': '(917)-000-0000', 'student3': '(470)-000-0000'}\n",
    "some_set = set( [1,2,4,4,5,5] )\n",
    "\n",
    "\n",
    "# In this part of the code I am using numpy (np) functions\n",
    "\n",
    "print (\"Square root: \" + str ( np.sqrt(25) ))\n",
    "print (\"Maximum element of our previous list: \" + str( np.max(some_list) ))\n",
    "\n",
    "# In this part of the code I am using python functions\n",
    "\n",
    "print (\"Number of elements in our previous list: \" + str( len(some_list) ))\n",
    "print (\"Sum of elements in our previous list: \" + str( sum(some_list) ))\n",
    "print (\"Range of 5 numbers (remember we start with 0): \" + str( range(5) ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the package **Pandas**? \n",
    "\n",
    "Pandas gives us the **DATAFRAME** -- one of the main data structures used in data analytics.\n",
    "\n",
    "A Dataframe is 2-dimensional \"labeled\" data structure with columns of potentially different types. It is generally the most commonly used pandas object. Along with the data, you can optionally pass index (row labels) and columns (column labels) arguments. It's often convenient to think of it as a spreadsheet with super powers! [More details here](http://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe)\n",
    "\n",
    "Pandas data frames can be constructed from most common data sources a data scientist will encounter: csv files, excel spreadsheets, sql databases, json, url pointers to other data sources, and even from other data already stored in one's python code. \n",
    "\n",
    "Let's take a look at creating a data frame from a common \"toy\" dataset presenting automobile mpg information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This reads the data from a url and sets the column names.\n",
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data-original\"\n",
    "column_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model', 'origin', 'car_name']\n",
    "mpg_df = pd.read_csv(url, delim_whitespace=True, header=None, names=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the data loaded in a pandas data frame, as a starter, let's see some of the (MANY!) ways pandas makes it convenient to explore a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, just get a peek at the data:\n",
    "mpg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some general stats about the data\n",
    "mpg_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info about the rows and columns themselves\n",
    "mpg_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many of each type of engine? \n",
    "mpg_df[\"cylinders\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total horsepower\n",
    "mpg_df[\"horsepower\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average horsepower per engine type\n",
    "mpg_df.groupby(\"cylinders\").horsepower.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of mpg\n",
    "mpg_df.hist(\"mpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or a scatter plot of acceleration vs mpg\n",
    "mpg_df.plot(kind=\"scatter\", x=\"acceleration\", y=\"mpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some pretty plotting comparing weight to mpg using regression in seaborn\n",
    "sns.jointplot(mpg_df, x=\"weight\", y=\"mpg\", kind=\"reg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is widely used and has a very active development community contributing new features. If there is some kind of analysis you want to do on your data, chances are, it already exists. I usually use google to find the information I need.\n",
    "\n",
    "One important component of pandas is indexing and selecting components of the data. This is a extremely rich topic, so i'll only touch on it here. Please [consult the documentation](https://pandas.pydata.org/pandas-docs/stable/indexing.html) for more info. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns can be selected using the `[]` operator, which accepts one column name or a list of several\n",
    "mpg_df[[\"cylinders\", \"car_name\"]].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As some \"syntactic sugar\", pandas also allows selection using the `.column_name` notation\n",
    "mpg_df.car_name.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that this can also be used for assignment!\n",
    "original_names = mpg_df.car_name.copy()\n",
    "mpg_df.car_name = original_names + \" Test!\"\n",
    "print(mpg_df.car_name.head())\n",
    "mpg_df.car_name = original_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For selecting rows from the data there are two options:\n",
    "- `.loc`: for selecting rows based on the _row label_\n",
    "- `.iloc`: for selecting rows based on the _row number_\n",
    "\n",
    "In the prior example, the row label and the row number are the same; often one wants to assign labels to rows, a unique id. In many cases, this would be something like a date or a user id. Note: these two selectors can also be used to pick columns, but I think that's a bit less common. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the 5th row.\n",
    "mpg_df.iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the first 5 rows\n",
    "mpg_df.iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have _actual labels_ as an index for a dataframe, we can use `.loc` to select using values from that index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_index_df = mpg_df.set_index(\"car_name\", inplace=False)\n",
    "car_index_df.loc[[\"amc rebel sst\", \"plymouth fury iii\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can also select those rows that match a particular condition. Say I want to only see those rows that have an acceleration less that 10 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_df[mpg_df.acceleration < 10].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, one wants to create a data frame from information that is available \"in code\"- these might be results of prior computations that aren't already in pandas or maybe just some small static dataframe that stores some info. There are two common ways to do this: lists-of-lists with an additional list of column names and lists of dictionaries. I prefer the latter since the data in this case is self-descriptive, order isn't important, and missing data is handled more smoothly, but I'll give examples below for both. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List-of-lists approach\n",
    "\n",
    "list1 = ['studentA',22,'(929)-000-000']\n",
    "list2 = ['studentB',np.nan,'(646)-000-000']\n",
    "list3 = ['studentC',30,'(917)-000-000']\n",
    "list4 = ['studentD',31,'(646)-001-001']\n",
    "list5 = ['studentE',np.nan,'(929)-001-001']\n",
    "list6 = ['studentF',30,'(917)-001-001']\n",
    "list7 = ['studentG',30,'(470)-001-001']\n",
    "\n",
    "list_of_lists = [list1, list2, list3, list4, list5, list6, list7]\n",
    "column_names = ['Name','Age','Mobile']\n",
    "\n",
    "lol_df = pd.DataFrame(list_of_lists,columns=column_names)\n",
    "lol_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the list of dicts approach\n",
    "alice = {\"name\": \"alice\", \"age\": 5, \"mobile\":\"555-222-9000\"}\n",
    "bob = {\"name\": \"bob\", \"age\": 100}\n",
    "casey = {\"age\":35, \"name\": \"casey\", \"mobile\":\"1-877-kars-4kids\"}\n",
    "\n",
    "list_of_dicts = [alice, bob, casey]\n",
    "lod_df = pd.DataFrame(list_of_dicts)\n",
    "lod_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add columns (they should have the same number of rows as the dataframe they are being added to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lol_df['Business Major'] = ['yes','no','yes','yes','yes','no','yes']\n",
    "lol_df['Years Experience'] = [1,4,2,6,0,3,0]\n",
    "lol_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "What about operations on entire columns? This can make data munging much easier!\n",
    "\n",
    "Let's take the difference between age and years of experience:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lol_df[\"Age\"] - lol_df[\"Years Experience\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the data frames used thus far have had missing values. We see that by default, pandas just displays `NaN`, when the value of a cell is unknown. Sometimes this interferes with the computation we're trying to accomplish. Fortunately, there is a [suite of functionality](https://pandas.pydata.org/pandas-docs/stable/missing_data.html) for dealing with missing data built in.\n",
    "\n",
    "Let's (for some reason) fill missing age info with the average age!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lol_df[\"Age\"].fillna(lol_df[\"Age\"].mean()) - lol_df[\"Years Experience\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra added bonus!!   ---  Auto-complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most useful things about IPython notebook is its tab completion. \n",
    "\n",
    "Try this: type `np.sqrt(` in the cell below and press `Shift + Tab` 4 times, slowly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I find this amazingly useful. I think of this as \"the more confused I am, the more times I should press Shift+Tab\". Nothing bad will happen if you tab 12 times.\n",
    "\n",
    "Okay, let's try tab completion for function names! Type `np.sq` then hit `Tab` when typing below to get suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is super useful when (like me) you forget the names of everything!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special Bonus 2! -- inline pyton help\n",
    "One of the coolest things about jupyter notebooks is that they give you access to the documentation of the objects and functions you're interacting with inside the notebook! just put a `?` at the beginning of the line, followed immediately by the thing you'd like help on:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?np.sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Taste of Whats to Come: Predictive Modeling\n",
    "We've seen thus far some examples of how jupyter, pandas and some other tools are great for manipulating and exploring data. This is great, but much of the power of data comes from its ability to help us predict future or unknown quantities. While this topic will be explored in much greater depth throughout the remainder of this class, let's take a sneak peek into some of what we'll be doing in the future. \n",
    "\n",
    "For this, let's build a simple model to predict the mpg of cars from the other information we have available on those cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we'll need to import the predictive model we'll use\n",
    "from sklearn import linear_model \n",
    "\n",
    "# Choose a particular sort of linear regression model\n",
    "# Get set up to train one of those\n",
    "my_linear = linear_model.Lasso(alpha=0.01)\n",
    "\n",
    "# Assemble the training data\n",
    "# Let's use these columns as features and the target variable\n",
    "features = [\"weight\", \"acceleration\", \"cylinders\", \"displacement\"]\n",
    "target = \"mpg\"\n",
    "\n",
    "# Eliminate (drop) any instances with missing values (NaNs) for now\n",
    "cleaned_df = mpg_df.dropna()\n",
    "\n",
    "# Train the model you set up on the data\n",
    "#   a.k.a. Fit the model to the data!\n",
    "my_linear.fit(cleaned_df[features], cleaned_df[target])\n",
    "\n",
    "# Show the coefficients of the linear model\n",
    "pd.DataFrame([dict(zip(features, my_linear.coef_))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get some predictions from the model\n",
    "preds = my_linear.predict(cleaned_df[features])\n",
    "predictions_df = cleaned_df.assign(predictions=preds)\n",
    "predictions_df[[\"mpg\", \"predictions\"]].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we visualize how good our predictions are?\n",
    "# Let's plot the predicted mpg vs. the true value\n",
    "predictions_df.plot(kind=\"scatter\", x=\"mpg\", y=\"predictions\")\n",
    "# Q: What would perfect predictions look like?\n",
    "# Q: How would you describe our predictive ability here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To master your new-found knowledge of Python, try these hands-on examples. \n",
    "\n",
    "Your homeworks will be in a similar format to this section.\n",
    "\n",
    "Consider the following URL to a CSV file containing the results of compressive tests for various types of concrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_url = \"https://www.openml.org/data/get_csv/1762521/phpZGl7F2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Load the CSV data into a pandas data frame. Print some high-level statistical info about the data frame's columns. HINT: You may also want to rename the columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_df = pd.read_csv(concrete_url)\n",
    "concrete_df.columns = ['Cement', 'Slag', 'Fly Ash', 'Water', 'Superplasticizer', 'Coarse Agg', 'Fine Agg', 'Age', 'MPa']\n",
    "print(concrete_df.describe())\n",
    "concrete_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. How many rows have a compressive strength > 40 MPa?|**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_df[concrete_df.MPa > 40].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Plot the histogram of Coarse Aggregate and Fine Aggregate values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_df.hist(\"Coarse Agg\")\n",
    "concrete_df.hist(\"Fine Agg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Make a plot comparing compressive strength to age**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_df.plot(\"MPa\", \"Age\", kind=\"scatter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Make a plot comparing compressive strength to age for only those rows with < 750 fine aggregate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_df[concrete_df[\"Fine Agg\"] < 750].plot(\"MPa\", \"Age\", kind=\"scatter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Try to build a linear model that predicts compressive strength given the other available fields**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_linear = linear_model.Lasso(alpha=0.01)\n",
    "features = ['Cement', 'Slag', 'Fly Ash', 'Water', 'Superplasticizer', 'Coarse Agg', 'Fine Agg', 'Age']\n",
    "target = \"MPa\"\n",
    "cleaned_df = concrete_df.dropna()\n",
    "my_linear.fit(cleaned_df[features], cleaned_df[target])\n",
    "pd.DataFrame([dict(zip(features, my_linear.coef_))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Generate predictions for all the observations and a scatterplot comparing the predicted compressive strengths to the actual values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = my_linear.predict(cleaned_df[features])\n",
    "predictions_df = cleaned_df.assign(predictions=preds)\n",
    "predictions_df.plot(kind=\"scatter\", x=\"MPa\", y=\"predictions\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
